import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import json
import time
import threading
# ================================= CONFIGURATION =================================
# 1. å¡«å…¥ä½ çš„Google Gemini APIå¯†é’¥
GOOGLE_API_KEY = "AIzaSyBoDQQLCJGe7Kz6cMdw2p1hGREMTauhwhM"

# 2. AnkiConnectçš„é…ç½®
ANKI_DECK_NAME = "æ–°æ¦‚å¿µè‹±è¯­2"  # ç›®æ ‡ç‰Œç»„
ANKI_MODEL_NAME = "èƒŒè¯¾æ–‡" # ç¬”è®°ç±»å‹åç§°
ANKI_CONNECT_URL = "http://127.0.0.1:8765"

# 3. Geminiæ¨¡å‹çš„é…ç½®
GENERATION_CONFIG = {
    "temperature": 0.5,
    "top_p": 1,
    "top_k": 1,
    "max_output_tokens": 4096,
}
# ===============================================================================

# ===============================================================================

# --- STAGE 1: WEB SCRAPER (ULTIMATE ROBUST VERSION) ---
def scrape_nce_lesson(lesson_url):
    """ä»ç»™å®šçš„URLçˆ¬å–æ–°æ¦‚å¿µè¯¾æ–‡ã€ç¿»è¯‘å’Œç”Ÿè¯ï¼ˆå·²å…¼å®¹æ–°æ¦‚å¿µ2å†Œæ‰€æœ‰å·²çŸ¥HTMLç»“æ„ï¼‰"""
    print(f"ğŸ•¸ï¸  æ­£åœ¨çˆ¬å–: {lesson_url}")
    try:
        response = requests.get(lesson_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
        response.raise_for_status()
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        content = {'english': '', 'chinese': '', 'vocabulary': ''}

        # --- é€šç”¨æå–å‡½æ•° (æ ¸å¿ƒæ”¹è¿›) ---
        def extract_content_after_h3(h3_text):
            h3_tag = soup.find('h3', string=h3_text)
            if not h3_tag: return ""
            
            parts = []
            # éå†h3ä¹‹åçš„æ‰€æœ‰åŒçº§èŠ‚ç‚¹
            for sibling in h3_tag.find_next_siblings():
                if sibling.name == 'h3': # é‡åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜å°±åœæ­¢
                    break
                # å¦‚æœæ˜¯<p>æ ‡ç­¾ï¼Œç›´æ¥è·å–æ–‡æœ¬
                if sibling.name == 'p':
                    parts.append(sibling.get_text(strip=True))
                # å¦‚æœæ˜¯divå®¹å™¨ï¼Œå°±è¿›å…¥å®¹å™¨æŸ¥æ‰¾<p>
                elif sibling.name == 'div':
                    for p_tag in sibling.find_all('p'):
                        parts.append(p_tag.get_text(strip=True))

            # å¦‚æœä¸Šè¿°æ–¹æ³•æ²¡æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•åœ¨h3çš„çˆ¶çº§å®¹å™¨é‡Œæ‰¾
            if not parts and h3_tag.parent.name == 'div':
                 for p_tag in h3_tag.parent.find_all('p'):
                     # ç¡®ä¿åªæ·»åŠ h3æ ‡é¢˜ä¹‹åçš„pæ ‡ç­¾
                     if p_tag.find_previous_sibling('h3') == h3_tag:
                         parts.append(p_tag.get_text(strip=True))

            return '\n\n'.join(parts)

        content['english'] = extract_content_after_h3('æ–°æ¦‚å¿µè‹±è¯­ï¼è¯¾æ–‡')
        content['chinese'] = extract_content_after_h3('æ–°æ¦‚å¿µè‹±è¯­ï¼ç¿»è¯‘')

        # --- æå–å•è¯ï¼ˆé€»è¾‘ä¿æŒï¼Œå› ä¸ºå®ƒæ¯”è¾ƒç¨³å®šï¼‰ ---
        h3_vocab = soup.find('h3', string='æ–°æ¦‚å¿µè‹±è¯­ï¼å•è¯å’ŒçŸ­è¯­')
        if h3_vocab:
            parent_container = h3_vocab.parent
            for br in parent_container.find_all('br'): br.replace_with('\n')
            full_text = parent_container.get_text(separator='\n', strip=True)
            vocab_text = full_text.split('æ–°æ¦‚å¿µè‹±è¯­ï¼å•è¯å’ŒçŸ­è¯­')[-1].strip()
            content['vocabulary'] = vocab_text.split('æ–°æ¦‚å¿µè‹±è¯­ï¼ç¿»è¯‘')[0].strip()

        if content['english'] and content['chinese']:
            print("âœ… çˆ¬å–æˆåŠŸã€‚")
            # print(f"   - [è°ƒè¯•] ä¸­æ–‡å†…å®¹ç‰‡æ®µ: {content['chinese'][:30]}...") # å–æ¶ˆæ³¨é‡Šä»¥è¿›è¡Œè°ƒè¯•
            return content
        else:
            print(f"âŒ çˆ¬å–å¤±è´¥: æœªæ‰¾åˆ°è‹±æ–‡è¯¾æ–‡æˆ–ä¸­æ–‡ç¿»è¯‘ã€‚")
            print(f"   - [è°ƒè¯•] è‹±æ–‡æ‰¾åˆ°: {'æ˜¯' if content['english'] else 'å¦'}")
            print(f"   - [è°ƒè¯•] ä¸­æ–‡æ‰¾åˆ°: {'æ˜¯' if content['chinese'] else 'å¦'}")
            return None
    except Exception as e:
        print(f"âŒ çˆ¬å–æˆ–è§£ææ—¶å‡ºé”™: {e}")
        return None

# --- STAGE 2: AI PROCESSING ---
def process_lesson_with_gemini(lesson_content):
    # ... (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œä¿æŒä¸Šä¸€ç‰ˆæœ¬çš„å³å¯)
    print("ğŸ¤– å¼€å§‹ä½¿ç”¨Geminiå¤„ç†å†…å®¹...")
    model = genai.GenerativeModel(model_name="gemini-1.5-flash", generation_config=GENERATION_CONFIG)
    stop_spinner = False
    def spinner():
        chars = "-\\|/"
        while not stop_spinner:
            for char in chars:
                print(f'\r   - æ­£åœ¨ç­‰å¾…Geminiå“åº” {char}', end='', flush=True)
                time.sleep(0.1)
        print('\r' + ' '*30 + '\r', end='')
    
    prompt_split = f"è¯·å°†ä»¥ä¸‹è‹±æ–‡è¯¾æ–‡å’Œä¸­æ–‡è¯‘æ–‡ï¼Œä¸€å¥å¯¹ä¸€å¥åœ°é…å¯¹èµ·æ¥ã€‚\nä¸¥æ ¼æŒ‰ç…§ \"è‹±æ–‡å¥å­ | ä¸­æ–‡å¥å­\" çš„æ ¼å¼è¾“å‡ºï¼Œæ¯ä¸€å¯¹å ä¸€è¡Œã€‚\n\nè‹±æ–‡è¯¾æ–‡:\n{lesson_content['english']}\n\nä¸­æ–‡è¯‘æ–‡:\n{lesson_content['chinese']}"
    sentence_pairs = []
    spinner_thread = threading.Thread(target=spinner)
    try:
        print("   - æ­£åœ¨å‘é€ã€æ™ºèƒ½åˆ†å¥ã€‘è¯·æ±‚...")
        spinner_thread.start()
        response = model.generate_content(prompt_split, request_options={"timeout": 120})
        stop_spinner = True
        spinner_thread.join()
        for line in response.text.strip().split('\n'):
            if '|' in line:
                eng, chn = line.split('|', 1)
                sentence_pairs.append((eng.strip(), chn.strip()))
        print(f"   - âœ… æ™ºèƒ½åˆ†å¥å®Œæˆï¼Œå…± {len(sentence_pairs)} å¥ã€‚")
    except Exception as e:
        stop_spinner = True
        spinner_thread.join()
        print(f"\n   - âŒ è°ƒç”¨Geminiè¿›è¡Œåˆ†å¥æ—¶å‡ºé”™ (å¯èƒ½æ˜¯è¶…æ—¶): {e}")
        return None

    all_notes_data = []
    total = len(sentence_pairs)
    for i, (eng, chn) in enumerate(sentence_pairs):
        prompt_note = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è‹±è¯­è€å¸ˆï¼Œä¸ºè¯æ±‡é‡çº¦1000-2000çš„åˆå­¦è€…è®²è§£æ–°æ¦‚å¿µè‹±è¯­ã€‚è®²è§£é£æ ¼éœ€ç®€æ˜æ‰¼è¦ã€å½¢è±¡ç”ŸåŠ¨ã€‚\næœ¬è¯¾ç”Ÿè¯: {lesson_content['vocabulary']}\nè¯·ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆç¬”è®°ï¼šè‹±æ–‡: \"{eng}\" ä¸­æ–‡: \"{chn}\"\nç¬”è®°å¿…é¡»ç”¨\"1. 2. 3.\"åˆ†ç‚¹è¯´æ˜ï¼ŒåŒ…å«: 1. **å¥å­ç»“æ„åˆ†æ** 2. **æ ¸å¿ƒè¯æ±‡/çŸ­è¯­** 3. **è¯­æ³•ç‚¹ç›**\nè¯·ç›´æ¥è¾“å‡ºç¬”è®°å†…å®¹ã€‚"
        stop_spinner = False
        spinner_thread = threading.Thread(target=spinner)
        try:
            print(f"   - ğŸ§  æ­£åœ¨ä¸ºç¬¬ {i+1}/{total} å¥ç”Ÿæˆç¬”è®°...")
            spinner_thread.start()
            note_response = model.generate_content(prompt_note, request_options={"timeout": 120})
            stop_spinner = True
            spinner_thread.join()
            all_notes_data.append({"english": eng, "chinese": chn, "note": note_response.text.strip()})
            print(f"   - âœ… ç¬¬ {i+1}/{total} å¥ç¬”è®°ç”ŸæˆæˆåŠŸã€‚")
        except Exception as e:
            stop_spinner = True
            spinner_thread.join()
            print(f"\n   - âŒ ä¸ºå¥å­ç”Ÿæˆç¬”è®°æ—¶å‡ºé”™ (å¯èƒ½æ˜¯è¶…æ—¶): {e}")
            all_notes_data.append({"english": eng, "chinese": chn, "note": "ç¬”è®°ç”Ÿæˆå¤±è´¥ã€‚"})
        time.sleep(1)
    print("âœ… Geminiå¤„ç†å®Œæˆã€‚")
    return all_notes_data

# --- STAGE 3: ANKI IMPORT (ENHANCED ERROR REPORTING) ---
def add_notes_to_anki(notes_data):
    """ä½¿ç”¨AnkiConnectçš„addNotesåŠ¨ä½œæ‰¹é‡å¯¼å…¥å¡ç‰‡ï¼ˆå·²ä¿®å¤NoneTypeå¹¶å¢å¼ºé”™è¯¯æŠ¥å‘Šï¼‰"""
    print("ğŸ“¤ å‡†å¤‡é€šè¿‡AnkiConnectæ‰¹é‡å¯¼å…¥å¡ç‰‡...")
    notes_to_add = [{"deckName": ANKI_DECK_NAME, "modelName": ANKI_MODEL_NAME, "fields": {"ä¸­æ–‡": note['chinese'], "è‹±æ–‡": note['english'], "ç¬”è®°": note['note'].replace('\n', '<br>')}, "tags": ["NCE_AutoImport", "NCE_Book2"]} for note in notes_data]
    payload = json.dumps({"action": "addNotes", "version": 6, "params": {"notes": notes_to_add}})
    try:
        response = requests.post(ANKI_CONNECT_URL, data=payload.encode('utf-8'))
        response_data = response.json()
        
        if response_data.get('error') is not None:
             print(f"âŒ Ankiå¯¼å…¥å¤±è´¥ (ä¸¥é‡é”™è¯¯): {response_data['error']}")
        else:
            result_list = response_data.get('result')
            if result_list is not None:
                success_count = sum(1 for r in result_list if isinstance(r, int))
                errors = [r for r in result_list if not isinstance(r, int)]
                print(f"ğŸ‰ Ankiå¯¼å…¥å®Œæˆï¼æˆåŠŸ: {success_count}å¼ ã€‚")
                if errors:
                    # åªæ˜¾ç¤ºç¬¬ä¸€æ¡è¯¦ç»†é”™è¯¯ï¼Œé¿å…åˆ·å±
                    print(f"   - å‡ºç° {len(errors)} ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚: '{errors[0]}'")
            else:
                print("   - AnkiConnectæœªè¿”å›æœ‰æ•ˆç»“æœï¼Œå¯èƒ½æ‰€æœ‰å¡ç‰‡éƒ½å·²å­˜åœ¨æˆ–å‘ç”ŸæœªçŸ¥é”™è¯¯ã€‚")

    except Exception as e:
        print(f"âŒ å‘é€æ•°æ®åˆ°Ankiæ—¶å‘ç”Ÿé”™è¯¯: {e}")

# ... (check_ankiconnect å’Œ main å‡½æ•°ä¿æŒä¸å˜) ...
def check_ankiconnect():
    try:
        requests.get(ANKI_CONNECT_URL, timeout=3)
        return True
    except requests.exceptions.RequestException:
        return False

def main():
    print("ğŸš€ å¯åŠ¨æ–°æ¦‚å¿µå…¨è‡ªåŠ¨å¯¼å…¥å·¥ä½œæµ ğŸš€")
    
    BOOK_TO_SCRAPE = 2
    START_LESSON = 1
    END_LESSON = 96
    
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–Geminiå¤±è´¥: {e}")
        return
    if not check_ankiconnect():
        print("âŒ AnkiConnectæœªè¿è¡Œã€‚è¯·å…ˆå¯åŠ¨Ankiæ¡Œé¢ç‰ˆã€‚")
        return

    print(f"ğŸ“š å‡†å¤‡å¤„ç†ã€Šæ–°æ¦‚å¿µè‹±è¯­ã€‹ç¬¬ {BOOK_TO_SCRAPE} å†Œï¼Œä»ç¬¬ {START_LESSON} è¯¾åˆ°ç¬¬ {END_LESSON} è¯¾...")
    base_url = "http://www.newconceptenglish.com/index.php?id=course"
    lesson_urls = [f"{base_url}-{BOOK_TO_SCRAPE}-{lesson:03d}" for lesson in range(START_LESSON, END_LESSON + 1)]
    
    for url in lesson_urls:
        print("\n" + "="*60)
        lesson_data = scrape_nce_lesson(url)
        if not lesson_data:
            print(f"è·³è¿‡æ­¤URLï¼Œè¿›å…¥ä¸‹ä¸€è¯¾...")
            time.sleep(2)
            continue
        
        anki_notes = process_lesson_with_gemini(lesson_data)
        if not anki_notes:
            print(f"Geminiå¤„ç†å¤±è´¥ï¼Œè·³è¿‡æ­¤è¯¾...")
            continue

        add_notes_to_anki(anki_notes)
        print(f"è¯¥è¯¾ç¨‹å¤„ç†å®Œæ¯•ï¼Œæš‚åœ15ç§’ä»¥é˜²APIè¯·æ±‚è¿‡å¿«...")
        time.sleep(15)

    print("\nğŸ æ‰€æœ‰æŒ‡å®šè¯¾ç¨‹å·²å¤„ç†å®Œæ¯•ï¼")


if __name__ == '__main__':
    main()