import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import json
import os
import time
from dotenv import load_dotenv
import concurrent.futures
from tqdm import tqdm

# --- é…ç½® ---
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# å¹¶è¡Œå¤„ç†é…ç½®
MAX_THREAD_WORKERS = 10
MAX_RETRIES_PER_TASK = 3
RETRY_DELAY_SECONDS = 5

# --- æ–‡ä»¶è·¯å¾„é…ç½® (MODIFIED) ---
# Reads individual files FROM this directory
INPUT_DIR = os.path.join("processed_data", "split_articles")
# Saves individual files TO this new directory
OUTPUT_DIR = os.path.join("processed_data", "final_notes")

# AIæ¨¡å‹é…ç½®
FLASH_MODEL_NAME = "gemini-2.5-flash"
PRO_MODEL_NAME = "gemini-2.5-pro"
GENERATION_CONFIG_DRAFT = {"temperature": 0.4}
GENERATION_CONFIG_REFINE = {"temperature": 0.5, "response_mime_type": "application/json"}

SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

# --- Prompts (Unchanged) ---
PROMPT_FOR_DRAFT_CET6 = """
ä½ æ˜¯ä¸€åé«˜æ•ˆçš„è‹±è¯­åŠ©æ•™ï¼Œä»»åŠ¡æ˜¯ä¸ºä¸€å¥å…­çº§ç¿»è¯‘çœŸé¢˜ç”Ÿæˆä¸€ä»½ç®€æ˜æ‰¼è¦çš„ç¬”è®°è‰ç¨¿ï¼Œä¸ºåç»­çš„ä¸“å®¶ç²¾è®²åšå‡†å¤‡ã€‚

**è¦æ±‚:**
1.  **æ ¸å¿ƒè¯æ±‡**: æç‚¼1-2ä¸ªå…³é”®ä¸­æ–‡è¡¨è¾¾åŠå…¶ç²¾å‡†çš„è‹±æ–‡ç¿»è¯‘ã€‚
2.  **ç¿»è¯‘äº®ç‚¹**: æŒ‡å‡ºè¯¥å¥ç¿»è¯‘ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæŠ€å·§æˆ–ç»“æ„ç‰¹ç‚¹ï¼ˆå¦‚ï¼šè¯­åºè°ƒæ•´ä¸ºå‰é‡åè½»ã€åˆ†è¯ä½œä¼´éšçŠ¶è¯­ã€æŠ½è±¡åè¯å…·ä½“åŒ–ç­‰ï¼‰ã€‚
3.  **ç®€æ´ç²¾ç‚¼**: è‰ç¨¿æ€»å­—æ•°æ§åˆ¶åœ¨50-80å­—ã€‚

**å¥å­ä¿¡æ¯:**
- ä¸­æ–‡: `{source_sentence_cn}`
- è‹±æ–‡å‚è€ƒ: `{reference_translation_en}`

**ä½ çš„è‰ç¨¿:**
"""

PROMPT_FOR_REFINEMENT_CET6 = """
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„å¤§å­¦è‹±è¯­å…­çº§ï¼ˆCET-6ï¼‰ç¿»è¯‘è¾…å¯¼ä¸“å®¶ï¼Œæ·±è°™å‡ºé¢˜è§„å¾‹å’Œé«˜åˆ†æŠ€å·§ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯åŸºäºæˆ‘æä¾›çš„**åŒä¸€è€ƒè¯•ä¸»é¢˜ä¸‹çš„å…¨æ–‡è¯­å¢ƒ**ã€**åˆæ­¥ç¬”è®°è‰ç¨¿**ä»¥åŠ**åå¤§ç¿»è¯‘å¿ƒæ³•**ï¼Œä¸ºæ¯ä¸€å¥è¯ç”Ÿæˆä¸€ä»½ä¸“å®¶çº§çš„æ·±åº¦ç²¾è®²ç¬”è®°ã€‚

**èƒŒæ™¯çŸ¥è¯† - CET-6å‡ºé¢˜è§„å¾‹ & åå¤§ç¿»è¯‘å¿ƒæ³•:**
1.  **å‡ºé¢˜è§„å¾‹**: åŒä¸€æ¬¡è€ƒè¯•çš„ä¸‰å¥—ç¿»è¯‘é¢˜é€šå¸¸å›´ç»•ä¸€ä¸ªå®å¤§ä¸»é¢˜ï¼ˆå¦‚ï¼šä¸­å›½ç§‘æŠ€æˆå°±ã€ä¼ ç»Ÿæ–‡åŒ–ã€ç¤¾ä¼šå‘å±•ï¼‰ï¼Œå› æ­¤æ–‡ç« é—´å­˜åœ¨å¤§é‡çš„è¯æ±‡å’Œå¥å¼å¤ç°ã€‚
2.  **åå¤§ç¿»è¯‘å¿ƒæ³•**:
    - **ç»“æ„ç±»**: æŠ€å·§ä¸€(ä¸‰æ­¥èµ°), æŠ€å·§äºŒ(å‰é‡åè½»), æŠ€å·§ä¸‰(ç»“æ„ä¸‰å‰‘å®¢: as, with, -ing)ã€‚
    - **è¯å¥ç±»**: æŠ€å·§å››(åŒä¹‰åˆå¹¶), æŠ€å·§äº”(åŒä¹‰æ›¿æ¢), æŠ€å·§ä¸ƒ(å‰¯è¯å–èˆ), æŠ€å·§å…«(åŠ¨è¯éšè—)ã€‚
    - **æ€ç»´ç±»**: æŠ€å·§å…­(èˆæ¦‚æ‹¬å–å…·ä½“), æŠ€å·§ä¹(åŒ–æŠ½è±¡ä¸ºå…·ä½“), æŠ€å·§å(å½¢å®¹è¯ç«‹ä½“åŒ–)ã€‚

**æ ¸å¿ƒæŒ‡ä»¤:**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼Œä¸ºæ¯ä¸ªå¥å­ç”Ÿæˆæœ€ç»ˆç¬”è®°ï¼š

1.  **å…¨å±€å…³è”åˆ†æ**:
    *   å®¡è§†â€œå…¨æ–‡è¯­å¢ƒâ€ï¼Œè¯†åˆ«å¹¶åˆ©ç”¨æœ¬æ¬¡è€ƒè¯•çš„**å®å¤§ä¸»é¢˜**ã€‚
    *   å¯¹æ¯”â€œæ‰€æœ‰å¥å­çš„ç¬”è®°è‰ç¨¿â€ï¼Œæ‰¾å‡ºè´¯ç©¿å…¨æ–‡çš„**ä¸»é¢˜è¯æ±‡**ï¼ˆå¦‚â€œå…»è€â€ã€â€œæ–‡åŒ–â€ã€â€œç§‘æŠ€â€ï¼‰å’Œ**é«˜é¢‘å¥å¼**ï¼ˆå¦‚â€œ...æ˜¯...ä¹‹ä¸€â€ã€â€œéšç€...â€ã€â€œä¸ä»…...è€Œä¸”...â€ï¼‰ã€‚

2.  **å•å¥æ·±åº¦ç²¾ç‚¼ (åº”ç”¨åå¤§å¿ƒæ³•)**:
    *   åœ¨è‰ç¨¿åŸºç¡€ä¸Šï¼Œå°†åˆ†æèå…¥â€œåå¤§ç¿»è¯‘å¿ƒæ³•â€ã€‚ä¾‹å¦‚ï¼Œçœ‹åˆ°è¯­åºè°ƒæ•´ï¼Œè¦ç‚¹æ˜è¿™æ˜¯**æŠ€å·§äºŒâ€œå‰é‡åè½»â€**çš„åº”ç”¨ï¼›çœ‹åˆ°åˆ†è¯çŸ­è¯­ï¼Œè¦æŒ‡å‡ºè¿™æ˜¯**æŠ€å·§ä¸‰â€œç»“æ„ä¸‰å‰‘å®¢â€**çš„å¦™ç”¨ã€‚
    *   **ã€å…³è”ã€‘æ˜¯å¼ºåˆ¶è¦æ±‚**: å¦‚æœå½“å‰å¥çš„çŸ¥è¯†ç‚¹ï¼ˆè¯æ±‡/å¥å¼/æŠ€å·§ï¼‰ä¸æœ¬æ®µå…¶ä»–å¥å­æœ‰å…³ï¼Œå¿…é¡»ä½¿ç”¨`ã€å…³è”ã€‘`æ ‡ç­¾æ˜ç¡®æŒ‡å‡ºã€‚ä¾‹å¦‚ï¼šâ€œã€å…³è”ã€‘æ­¤å¤„çš„â€˜make... more accessibleâ€™å‘¼åº”äº†æŠ€å·§ä¹â€˜åŒ–æŠ½è±¡ä¸ºå…·ä½“â€™ï¼Œä¸ç¬¬ä¸‰å¥å¤„ç†â€˜å…»è€æœåŠ¡â€™çš„æ€è·¯ä¸€è‡´ã€‚â€
    *   **ä¸¥æ ¼éµå¾ªPRDä¸‰æ®µå¼ç»“æ„**:
        *   `ã€æ ¸å¿ƒè¯æ±‡ä¸è¡¨è¾¾è§£æã€‘`: ç²¾è®²æ ¸å¿ƒè¯æ±‡ï¼Œå¹¶å…³è”å…¨æ–‡ä¸»é¢˜è¯ã€‚
        *   `ã€å¥æ³•åˆ†æä¸ç¿»è¯‘æŠ€å·§ã€‘`: ç»“åˆâ€œåå¤§å¿ƒæ³•â€å‰–æå¥å­ç»“æ„å’Œç¿»è¯‘ç­–ç•¥ã€‚
        *   `ã€å¯æ›¿æ¢è¡¨è¾¾ä¸æ‹“å±•ã€‘`: æä¾›é«˜è´¨é‡çš„åŒä¹‰æ›¿æ¢ï¼Œä½“ç°æŠ€å·§äº”â€œåƒå˜ä¸‡åŒ–â€ã€‚

**è¾“å…¥ä¿¡æ¯:**

**1. å…¨æ–‡è¯­å¢ƒ (åŒä¸€è€ƒè¯•ä¸»é¢˜ä¸‹çš„ä¸‰ç¯‡æ–‡ç« ä¹‹ä¸€):**
- ä¸­æ–‡å…¨æ–‡: `{full_context_cn}`
- è‹±æ–‡å…¨æ–‡: `{full_context_en}`

**2. æ‰€æœ‰å¥å­çš„ç¬”è®°è‰ç¨¿:**{all_drafts_text}

**è¾“å‡ºæŒ‡ä»¤:**
ä½ çš„æœ€ç»ˆè¾“å‡º**å¿…é¡»**æ˜¯ä¸€ä¸ª**çº¯ç²¹çš„ã€ä¸å«ä»»ä½•å…¶ä»–æ–‡æœ¬çš„ã€æ ¼å¼æ­£ç¡®çš„JSONæ•°ç»„**ã€‚æ•°ç»„ä¸­çš„æ¯ä¸ªå¯¹è±¡ä»£è¡¨ä¸€ä¸ªå¥å­çš„æœ€ç»ˆç¬”è®°ï¼Œè¦æ±‚å†…å®¹ç²¾ç‚¼æ·±åˆ»ï¼Œå•æ¡ç¬”è®°å­—æ•°åœ¨100å­—ä»¥å†…ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
`[ {{ "sentence_index": 1, "final_note": "ã€æ ¸å¿ƒè¯æ±‡...ã€‘...ã€å…³è”ã€‘..." }}, {{ "sentence_index": 2, "final_note": "..." }}]`

è¯·ç°åœ¨å¼€å§‹ç”Ÿæˆè¿™ä¸ªJSONæ•°ç»„ã€‚
"""

def generate_draft_for_sentence(args):
    """(é˜¶æ®µä¸€) ä¸ºå•ä¸ªå¥å­ç”Ÿæˆç¬”è®°è‰ç¨¿"""
    sentence_data, model = args
    prompt = PROMPT_FOR_DRAFT_CET6.format(**sentence_data)
    for attempt in range(MAX_RETRIES_PER_TASK):
        try:
            response = model.generate_content(prompt)
            return {"sentence_index": sentence_data["sentence_index"], "draft_note": response.text.strip()}
        except Exception as e:
            if attempt == MAX_RETRIES_PER_TASK - 1:
                return {"sentence_index": sentence_data["sentence_index"], "draft_note": f"è‰ç¨¿ç”Ÿæˆå¤±è´¥: {e}"}
            time.sleep(RETRY_DELAY_SECONDS)

def main():
    print("ğŸš€ æ¨¡å— 2b: å¼€å§‹ç”ŸæˆAIç¬”è®° (ä¸¤é˜¶æ®µ)... ğŸš€")

    if not GOOGLE_API_KEY:
        print("âŒ é”™è¯¯ï¼šæœªåœ¨.envæ–‡ä»¶ä¸­æ‰¾åˆ°GOOGLE_API_KEYã€‚")
        return
    genai.configure(api_key=GOOGLE_API_KEY)

    # --- Create output directory ---
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # --- Get list of files to process ---
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥ç›®å½• {INPUT_DIR}ã€‚è¯·å…ˆè¿è¡Œæ¨¡å— 2aã€‚")
        return
    
    files_to_process = sorted([f for f in os.listdir(INPUT_DIR) if f.endswith('.json')])
    if not files_to_process:
        print(f"ğŸŸ¡ åœ¨ç›®å½• {INPUT_DIR} ä¸­æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„JSONæ–‡ä»¶ã€‚")
        return
        
    print(f"âœ… æ‰¾åˆ° {len(files_to_process)} ä¸ªå·²åˆ†å¥çš„æ–‡ä»¶å‡†å¤‡å¤„ç†ã€‚")

    flash_model = genai.GenerativeModel(FLASH_MODEL_NAME, generation_config=GENERATION_CONFIG_DRAFT, safety_settings=SAFETY_SETTINGS)
    pro_model = genai.GenerativeModel(PRO_MODEL_NAME, generation_config=GENERATION_CONFIG_REFINE, safety_settings=SAFETY_SETTINGS)
    
    success_count, skipped_count = 0, 0

    # --- Main loop iterates over FILENAMES ---
    for filename in tqdm(files_to_process, desc="å¤„ç†æ‰€æœ‰çœŸé¢˜æ–‡ä»¶"):
        input_filepath = os.path.join(INPUT_DIR, filename)
        output_filepath = os.path.join(OUTPUT_DIR, filename)

        # Skip if already processed
        if os.path.exists(output_filepath):
            continue

        with open(input_filepath, 'r', encoding='utf-8') as f:
            item = json.load(f)
        
        print(f"\nğŸ“„ æ­£åœ¨å¤„ç†: {item['id']} - {item['title']}")
        
        # --- é˜¶æ®µä¸€: å¹¶è¡Œç”Ÿæˆè‰ç¨¿ ---
        tasks = [(sentence, flash_model) for sentence in item["sentences"]]
        drafts = {}
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREAD_WORKERS) as executor:
            results = list(tqdm(executor.map(generate_draft_for_sentence, tasks), total=len(tasks), desc="  - (1/2) ç”Ÿæˆè‰ç¨¿ (Flash)", leave=False))
            for res in results:
                drafts[res["sentence_index"]] = res["draft_note"]
        
        # --- é˜¶æ®µäºŒ: æ•´ä½“ç²¾ç‚¼ä¸å…³è” ---
        print("  - (2/2) æ­£åœ¨ç²¾ç‚¼ä¸å…³è” (Pro)...")
        all_drafts_text = "\n\n".join([f"--- å¥å­ {i} ---\n{drafts.get(i, 'N/A')}" for i in sorted(drafts.keys())])
        
        refinement_prompt_filled = PROMPT_FOR_REFINEMENT_CET6.format(
            full_context_cn=item["full_context_cn"],
            full_context_en=item["full_context_en"],
            all_drafts_text=all_drafts_text
        )

        final_notes_list = None
        for attempt in range(MAX_RETRIES_PER_TASK):
            try:
                response = pro_model.generate_content(refinement_prompt_filled, request_options={"timeout": 300})
                final_notes_list = json.loads(response.text)
                break 
            except Exception as e:
                print(f"    - âš ï¸ ç²¾ç‚¼å°è¯• {attempt + 1}/{MAX_RETRIES_PER_TASK} å¤±è´¥: {e}")
                if attempt < MAX_RETRIES_PER_TASK - 1:
                    time.sleep(RETRY_DELAY_SECONDS)
                else:
                    print(f"    - âŒ è·³è¿‡æ­¤é¢˜çš„ç¬”è®°ç”Ÿæˆã€‚")
        
        if final_notes_list:
            final_notes_map = {note["sentence_index"]: note["final_note"] for note in final_notes_list}
            for sentence in item["sentences"]:
                sentence["ai_note"] = final_notes_map.get(sentence["sentence_index"], "æœ€ç»ˆç¬”è®°ç”Ÿæˆå¤±è´¥")
            
            # --- START OF CHANGE ---
            # Create a new dictionary containing only the 'sentences' key and its data
            output_data = {"sentences": item["sentences"]}
            
            # Save the new, simplified data structure to the file
            with open(output_filepath, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            # --- END OF CHANGE ---
            
            success_count += 1
            print("  - âœ… ç¬”è®°ç”ŸæˆæˆåŠŸã€‚")
        else:
            skipped_count += 1

    print(f"\nğŸ AIç¬”è®°ç”Ÿæˆå…¨éƒ¨å®Œæˆï¼æˆåŠŸå¤„ç† {success_count} ä¸ªæ–‡ä»¶, è·³è¿‡/å¤±è´¥ {skipped_count} ä¸ªã€‚ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_DIR}")

if __name__ == '__main__':
    main()