# 2a_split_sentences.py

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import json
import os
import time
from dotenv import load_dotenv
from tqdm import tqdm
import re

# --- é…ç½® ---
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# é‡è¯•é…ç½®
MAX_RETRIES_PER_ITEM = 3
RETRY_DELAY_SECONDS = 5

# æ–‡ä»¶è·¯å¾„é…ç½®
RAW_DATA_PATH = os.path.join("raw_data", "cet6_raw_data.json")
SPLIT_DATA_PATH = os.path.join("processed_data", "cet6_split_data.json") # è¾“å‡ºæ–‡ä»¶

# AIæ¨¡å‹é…ç½® (åˆ†å¥ä»»åŠ¡ç”¨Flashå³å¯)
MODEL_NAME = "gemini-2.5-flash" 
GENERATION_CONFIG = {"temperature": 0.0, "max_output_tokens": 4096}
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

# --- ä¸“ä¸ºåˆ†å¥è®¾è®¡çš„Prompt ---
PROMPT_FOR_SPLITTING = """
ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸€æ®µä¸­æ–‡æ–‡æœ¬å’Œå…¶å¯¹åº”çš„è‹±æ–‡ç¿»è¯‘ï¼Œç²¾å‡†åœ°æ‹†åˆ†æˆä¸€ä¸€å¯¹åº”çš„å¥å­å¯¹ã€‚

**æ ¸å¿ƒè§„åˆ™:**
1.  **ä»¥è‹±æ–‡ä¸ºå‡†**: ä¸¥æ ¼ä»¥è‹±æ–‡æ–‡æœ¬çš„å¥å­ç»“æŸç¬¦ï¼ˆ. ?!ï¼‰ä½œä¸ºåˆ‡åˆ†åŸºå‡†ã€‚è‹±æ–‡çš„ä¸€ä¸ªå®Œæ•´å¥å­å¿…é¡»å¯¹åº”ä¸€è¡Œè¾“å‡ºã€‚
2.  **åŒ¹é…ä¸­æ–‡**: å°†ä¸­æ–‡æ–‡æœ¬è¿›è¡Œå¿…è¦çš„ã€æœ€å°åŒ–çš„åˆ‡åˆ†æˆ–è°ƒæ•´ï¼Œä½¿å…¶åœ¨è¯­ä¹‰ä¸Šä¸å¯¹åº”çš„è‹±æ–‡å¥å­å¯¹é½ã€‚
3.  **ä¸¥æ ¼æ ¼å¼**: ä½ çš„è¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªå¥å­å¯¹ï¼Œä¸å¾—æœ‰ä»»ä½•é¢å¤–è§£é‡Šï¼š
    `å¥å­åºå· | ä¸­æ–‡å†…å®¹ | è‹±æ–‡å†…å®¹`

**ç¤ºä¾‹:**
è¾“å…¥ä¸­æ–‡: "ä¸­å›½çš„å®¶åº­è§‚å¿µä¸å…¶æ–‡åŒ–ä¼ ç»Ÿå¯†åˆ‡ç›¸å…³ã€‚å®¶åº­å’Œç¦è¢«è§†ä¸ºå¤´ç­‰å¤§äº‹ã€‚"
è¾“å…¥è‹±æ–‡: "China's concept of family is closely related to its cultural traditions. Family harmony is regarded as a top priority."
è¾“å‡º:
1 | ä¸­å›½çš„å®¶åº­è§‚å¿µä¸å…¶æ–‡åŒ–ä¼ ç»Ÿå¯†åˆ‡ç›¸å…³ã€‚ | China's concept of family is closely related to its cultural traditions.
2 | å®¶åº­å’Œç¦è¢«è§†ä¸ºå¤´ç­‰å¤§äº‹ã€‚ | Family harmony is regarded as a top priority.

---
**å¾…å¤„ç†æ–‡æœ¬:**

**ä¸­æ–‡æ®µè½:**
{text_cn}

**è‹±æ–‡æ®µè½:**
{text_en}

**è¯·ä¸¥æ ¼æŒ‰æ ¼å¼è¾“å‡º:**
"""

def split_text_into_sentences_advanced(text_cn, text_en, model):
    """
    ä½¿ç”¨AIå°†ä¸­è‹±æ–‡æ®µè½æ‹†åˆ†ä¸ºå¸¦ç´¢å¼•çš„å¥å­å¯¹ã€‚
    å¦‚æœå¤±è´¥ï¼Œè¿”å›Noneã€‚
    """
    prompt = PROMPT_FOR_SPLITTING.format(text_cn=text_cn, text_en=text_en)
    for attempt in range(MAX_RETRIES_PER_ITEM):
        try:
            response = model.generate_content(prompt)
            if not response.parts:
                raise ValueError(f"APIå“åº”ä¸ºç©º, åŸå› : {response.candidates[0].finish_reason}")
            
            text_output = response.text.strip()
            sentence_pairs = []
            lines = text_output.split('\n')
            
            # æ ¡éªŒè¾“å‡ºæ ¼å¼æ˜¯å¦åŸºæœ¬æ­£ç¡®
            if not lines or '|' not in lines[0]:
                 raise ValueError(f"è¾“å‡ºæ ¼å¼ä¸æ­£ç¡®ï¼Œä¸åŒ…å«'|': {text_output[:100]}")

            for line in lines:
                parts = line.split('|', 2)
                if len(parts) == 3:
                    index_str = parts[0].strip()
                    cn_sent = parts[1].strip()
                    en_sent = parts[2].strip()
                    
                    # å†æ¬¡æ ¡éªŒæ•°æ®æœ‰æ•ˆæ€§
                    if index_str.isdigit() and cn_sent and en_sent:
                        sentence_pairs.append({
                            "sentence_index": int(index_str),
                            "source_sentence_cn": cn_sent,
                            "reference_translation_en": en_sent
                        })
                    else: # å¦‚æœæŸä¸€è¡Œè§£æå¤±è´¥ï¼Œåˆ™è®¤ä¸ºæ•´ä¸ªåˆ†å¥å¤±è´¥
                        raise ValueError(f"è§£æè¡Œå¤±è´¥: {line}")
                else:
                    raise ValueError(f"è¡Œä¸ç¬¦åˆ'ç´¢å¼•|ä¸­|è‹±'æ ¼å¼: {line}")
            
            if not sentence_pairs:
                raise ValueError("æˆåŠŸè§£æåï¼Œå¥å­å¯¹åˆ—è¡¨ä¸ºç©ºã€‚")

            return sentence_pairs # æˆåŠŸï¼Œè¿”å›åˆ—è¡¨

        except Exception as e:
            print(f"   - âš ï¸ åˆ†å¥å°è¯• {attempt + 1}/{MAX_RETRIES_PER_ITEM} å¤±è´¥: {e}")
            if attempt < MAX_RETRIES_PER_ITEM - 1:
                time.sleep(RETRY_DELAY_SECONDS)
            else:
                print(f"   - âŒ å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåˆ†å¥å¤±è´¥ã€‚")
                return None # æœ€ç»ˆå¤±è´¥ï¼Œè¿”å›None
    return None

# 2a_split_sentences.py -> main() å‡½æ•°çš„ä¿®æ­£ç‰ˆ

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("ğŸš€ æ¨¡å— 2a: å¼€å§‹æ™ºèƒ½åˆ†å¥... ğŸš€")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs("processed_data", exist_ok=True)
    
    genai.configure(api_key=GOOGLE_API_KEY)
    
    try:
        with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(raw_data)} å¥—ç¿»è¯‘çœŸé¢˜ã€‚")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°åŸå§‹æ•°æ®æ–‡ä»¶ {RAW_DATA_PATH}ã€‚è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
        return
    except json.JSONDecodeError:
        print(f"âŒ é”™è¯¯: åŸå§‹æ•°æ®æ–‡ä»¶ {RAW_DATA_PATH} æ ¼å¼ä¸æ­£ç¡®ã€‚")
        return

    model = genai.GenerativeModel(MODEL_NAME, generation_config=GENERATION_CONFIG, safety_settings=SAFETY_SETTINGS)
    
    final_split_data, skipped_count = [], 0
    
    # --- ä¿®æ”¹ç‚¹: ä½¿ç”¨ enumerate ä¸ºæ•°æ®æ·»åŠ ç´¢å¼• ---
    for index, item in enumerate(tqdm(raw_data, desc="å¤„ç†æ‰€æœ‰çœŸé¢˜")):
        
        # --- ä¿®æ”¹ç‚¹: å®‰å…¨åœ°è·å–idå’Œtitleï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç´¢å¼•ç”Ÿæˆ ---
        item_id = item.get('id', f"cet6_{index + 1:03d}") 
        item_title = item.get('title', f"çœŸé¢˜_{index + 1}")
        
        print(f"\nğŸ“„ æ­£åœ¨åˆ†å¥: {item_id} - {item_title}") # ç°åœ¨è¿™ä¸ªprintè¯­å¥å¯ä»¥å®‰å…¨æ‰§è¡Œ
        
        sentence_pairs = split_text_into_sentences_advanced(item['source_text_cn'], item['reference_translation_en'], model)
        
        if sentence_pairs:
            final_split_data.append({
                "id": item_id, 
                "title": item_title, 
                "full_context_cn": item['source_text_cn'],
                "full_context_en": item['reference_translation_en'], 
                "sentences": sentence_pairs
            })
        else:
            skipped_count += 1
            print(f"   - â— {item_id} - {item_title} å¤„ç†å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
            
    with open(SPLIT_DATA_PATH, 'w', encoding='utf-8') as f:
        json.dump(final_split_data, f, ensure_ascii=False, indent=2)
        
    print(f"\nğŸ æ™ºèƒ½åˆ†å¥å…¨éƒ¨å®Œæˆï¼æˆåŠŸ: {len(final_split_data)}, å¤±è´¥è·³è¿‡: {skipped_count}ã€‚ç»“æœä¿å­˜è‡³: {SPLIT_DATA_PATH}")

if __name__ == '__main__':
    main()