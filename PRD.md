
#  **产品需求文档 (PRD): 自动化语言学习卡片生成系统**

**版本:** 1.0
**日期:** 2023年10月27日
**作者:** (AI Assistant)

---

### 1. **简介与愿景**

#### 1.1 项目概述
本项目旨在构建一个自动化的语言学习内容处理管道。该系统能从指定的双语文本源（如新概念英语网站、六级翻译真题库等）获取原始材料，利用大型语言模型（LLM）进行智能化的内容解析、注释生成和知识点关联，最终生成适合导入间隔重复系统（SRS，如Anki）的高质量、带发音的学习卡片。

#### 1.2 问题陈述
传统的语言学习者在制作个性化学习卡片时面临以下痛点：
*   **耗时耗力:** 手动复制、粘贴、排版和查找释义的过程极为繁琐。
*   **质量不均:** 个人制作的笔记可能缺乏深度、系统性和关联性，学习效率不高。
*   **缺乏上下文:** 孤立的单词或句子卡片难以帮助学习者建立知识网络。
*   **制作音频困难:** 为每张卡片添加标准发音需要额外的工具和步骤。

#### 1.3 系统愿景
成为一个灵活、可扩展的内容处理引擎，能够将任何结构化的双语文本转化为富有洞察力、上下文关联紧密且易于复习的互动学习材料，从而极大地提升语言学习的效率和深度。

---

### 2. **系统架构与工作流程**

系统由三个核心模块组成，形成一个清晰的数据处理流水线：

**[数据源] -> [模块1: 数据采集与标准化] -> [模块2: AI内容处理与增强] -> [模块3: Anki集成与发布] -> [用户]**



*   **模块1 (对应 `1_scrape_lessons.py`):** 负责从各种数据源抓取或读取双语文本，并将其清洗、格式化为包含元数据的标准JSON格式。
*   **模块2 (对应 `2_process_with_ai.py`):** 系统的“大脑”。接收标准JSON，进行句子切分，并调用AI模型为每个句子生成深度解析笔记。
*   **模块3 (对应 `3_import_to_anki.py`):** 系统的“执行者”。将AI处理过的数据，结合TTS音频生成，批量、智能地导入到Anki中。

---

### 3. **模块详细需求**

#### 3.1 **模块1: 数据采集与标准化 (Data Acquisition & Standardization)**

**目标:** 将任何来源的双语内容，统一转换为结构化的JSON文件，为下游处理提供干净、一致的数据输入。

**功能需求:**
1.  **可配置的数据源:** 系统应支持通过简单的配置来切换不同的数据源（例如，通过URL模板、本地文件路径等）。
2.  **内容提取:**
    *   **核心字段:** 必须能稳定提取三个核心内容字段：`english_text` (英文原文), `chinese_text` (中文译文), `supplementary_info` (补充信息，如生词表、背景知识等)。
    *   **健壮性:** 包含重试机制和错误处理，确保在网络不稳定或页面结构微小变化时的数据采集成功率。
3.  **元数据注入:** 在输出的JSON中，必须包含丰富的元数据，以便于索引、上下文关联和未来的功能扩展。
4.  **标准化的输出:** 所有数据源经过此模块处理后，都应输出为统一的JSON格式。

**输出JSON数据结构 (`raw_data/lesson_xxx.json`):**
```json
{
  "metadata": {
    "source": "New Concept English", // 数据来源, e.g., "CET-6 Translation"
    "type": "Lesson Text",          // 内容类型, e.g., "Past Exam Question"
    "book_number": 3,               // 册号 (可选)
    "lesson_number": 1,             // 课号/篇号
    "title": "A puma at large",     // 篇章标题
    "url": "http://..."             // 原始链接 (可选)
  },
  "content": {
    "english_text": "Pumas are large, cat-like animals which are found in America...",
    "chinese_text": "美洲狮是一种像猫一样的大动物，产于美洲...",
    "supplementary_info": "puma n.美洲狮\nspot v.看到，发现\n..."
  }
}
```

---

#### 3.2 **模块2: AI内容处理与增强 (AI Content Processing & Enhancement)**

**目标:** 将篇章文本分解为句子单元，并利用AI为每个句子生成高质量、上下文感知的学习笔记。

**功能需求:**
1.  **句子分割 (Sentence Segmentation):**
    *   **规则优先:** 必须实现一个更可靠的句子分割逻辑。应**优先基于英文的标点符号（`.`, `?`, `!`）** 来进行切分。
    *   **智能对齐:** 切分英文后，系统需要智能地将中文译文与切分好的英文句子进行一对一的精确对齐。这可能需要一个额外的AI调用或复杂的算法来保证准确性。
2.  **添加句子索引:** 在处理后的数据中，为每个句子对添加一个从1开始的`sentence_index`字段。这对于AI在后续步骤中理解句子顺序和进行上下文关联至关重要。
3.  **两阶段AI笔记生成:**
    *   **阶段一 (并行草稿生成):** 使用高效率模型（如Gemini Flash），为每个句子并行生成初步的讲解笔记（词汇、句型分析、举一反三）。
    *   **阶段二 (串行精炼与关联):** 使用高性能模型（如Gemini Pro），按句子顺序**依次**处理。在处理当前句子时，向AI提供 **“全文所有句子的笔记草稿”** 和 **“在此之前已精炼好的笔记”** 作为双重上下文，使其能够：
        *   **精炼**语言，使其更简洁生动。
        *   **发现**并指出当前知识点与前后文知识点的**关联**。
        *   **避免**重复讲解已经提过的知识点。
4.  **健壮的API调用:** 包含重试、超时和错误捕获机制，确保与AI服务的稳定通信。

**输出JSON数据结构 (`processed_data/lesson_xxx.json`):**
```json
[
  {
    "sentence_index": 1,
    "english": "Pumas are large, cat-like animals which are found in America.",
    "chinese": "美洲狮是一种像猫一样的大动物，产于美洲。",
    "note": "1. 词汇与短语: puma (n.) 美洲狮, cat-like (adj.) 像猫一样的。\n2. 句子结构分析: 这是一个“主系表”结构（Pumas are animals），后面跟着一个which引导的定语从句，用来修饰animals，告诉我们这些动物“在哪里被发现”。\n3. 举一反三: Pandas are bear-like animals which are found in China."
  },
  {
    "sentence_index": 2,
    "english": "When reports came into London Zoo that a wild puma had been spotted forty-five miles south of London, they were not taken seriously.",
    "chinese": "当伦敦动物园接到报告说，在伦敦以南45英里处发现一只美洲狮时，这些报告并没有受到重视。",
    "note": "1. 句子结构分析: 这是一个复合句。主句是 'they were not taken seriously' (主谓结构，被动语态)。'When...'引导的是时间状语从句，'that...'引导的是同位语从句，解释reports的内容。\n2. 【关联点】: 注意这里的被动语态 'were not taken'，和第一句的 'are found' 类似，都强调动作的承受者而不是发出者。"
  }
]
```

---

#### 3.3 **模块3: Anki集成与发布 (Anki Integration & Publishing)**

**目标:** 将处理好的笔记数据无缝、智能地导入Anki，并完成TTS音频的自动生成和配置。

**功能需求:**
1.  **防重复导入:**
    *   **基于标签的查询:** 在导入前，必须通过查询特定格式的标签（如`NCE3-Lesson1`）来获取该课文在Anki中已存在的所有卡片。
    *   **基于内容的匹配:** 将查询到的卡片英文内容与待导入的句子进行比对，精确跳过已存在的卡片，实现幂等性导入（重复运行脚本不会产生重复卡片）。
2.  **高质量TTS音频:**
    *   集成Google Cloud Text-to-Speech服务，为每张卡片的英文句子生成高质量、发音自然的MP3音频。
    *   应包含重试逻辑以应对可能的网络或服务暂时性问题。
3.  **自动化卡片创建:**
    *   调用AnkiConnect API，将英文、中文、AI笔记和音频文件路径填充到Anki指定模板的对应字段中。
    *   **自动添加标签:** 为每张卡片自动添加层级化的标签，如`NCE_Book3`（总标签）和`NCE3-Lesson1`（课文标签），便于后续的筛选和管理。
4.  **导入后处理:** 成功导入一个JSON文件后，应将其移动到`imported`子文件夹，以避免下次运行时重复处理，并清晰地标识已完成的任务。

---

### 4. **非功能性需求**

*   **可配置性:** 所有关键参数（API密钥、书册编号、Anki牌组/模板名、模型名称、文件路径等）都应通过外部配置文件（如`.env`）或脚本顶部的常量进行管理，而非硬编码。
*   **性能:** 在AI处理阶段，应利用并发（多线程/多进程）来提高效率，尤其是在生成笔记草稿和处理多个文件时。
*   **错误处理与日志:** 每个模块都应有清晰的日志输出，记录关键步骤、成功、警告和错误信息，便于追踪和调试问题。
*   **安全性:** API密钥等敏感信息必须通过环境变量等安全方式进行管理，不能提交到代码仓库。

---

### 5. **未来路线图 (Roadmap)**

*   **V1.1:** 增加对本地文件（如TXT, Markdown）作为数据源的支持。
*   **V1.2:** 开发一个简单的命令行界面（CLI），让用户可以更方便地指定任务（如`python main.py --source cet6 --range 2020-2022`）。
*   **V2.0:** 引入图形用户界面（GUI），用户可以通过界面管理数据源、监控处理进度和预览生成的卡片。
*   **V2.1:** 扩展对更多SRS平台的支持，如通过生成标准CSV文件进行通用导入。
*   **V2.2:** 允许用户在处理前自定义AI的Prompt，以调整生成笔记的风格和侧重点。