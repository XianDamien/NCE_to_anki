# 1_scrape_lessons.py

import requests
from bs4 import BeautifulSoup
import json
import os
import time

# --- é…ç½® ---
BOOK_TO_SCRAPE = 3 # <<< æ”¹åŠ¨ï¼šç°åœ¨ç›®æ ‡æ˜¯ç¬¬ä¸‰å†Œ >>>
TOTAL_LESSONS = 60 # <<< æ”¹åŠ¨ï¼šç¬¬ä¸‰å†Œå…±60è¯¾ >>>
BASE_URL = "http://www.newconceptenglish.com/index.php"
OUTPUT_DIR = os.path.join("raw_data", f"nce_book_{BOOK_TO_SCRAPE}")
MAX_ATTEMPTS = 3 # æ¯ç¯‡è¯¾æ–‡æœ€å¤šå°è¯•æ¬¡æ•°
# ------------

def get_text_until_next_h3(start_tag):
    """ä¸€ä¸ªå¥å£®çš„è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæŠ“å–ä»ä¸€ä¸ª<h3>æ ‡ç­¾å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€ä¸ª<h3>æ ‡ç­¾ä¹‹å‰çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹"""
    if not start_tag: return ""
    
    content_parts = []
    for element in start_tag.next_siblings:
        if element.name == 'h3':
            break
        text = element.get_text(" ", strip=True) 
        if text:
            content_parts.append(text)
    return " ".join(content_parts).strip()

def scrape_nce_lesson(lesson_url):
    """
    æ ¸å¿ƒçˆ¬è™«å‡½æ•°ï¼šä¸ºNCEç¬¬ä¸‰å†Œå®šåˆ¶ï¼Œå¹¶æŠ“å–æ‰€æœ‰å¿…è¦å­—æ®µã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«å†…å®¹çš„å­—å…¸ï¼Œæˆ–åœ¨å¤±è´¥æ—¶è¿”å›Noneã€‚
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(lesson_url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # <<< æ ¸å¿ƒæ”¹åŠ¨ï¼šå­—æ®µåä¸æˆ‘ä»¬AIè„šæœ¬æ‰€éœ€çš„ä¸€è‡´ >>>
        content = {'english': '', 'chinese': '', 'vocabulary': ''}
        
        # 1. æå–è‹±æ–‡è¯¾æ–‡
        h3_english = soup.find('h3', string='æ–°æ¦‚å¿µè‹±è¯­ï¼è¯¾æ–‡')
        content['english'] = get_text_until_next_h3(h3_english)

        # 2. æå–ä¸­æ–‡ç¿»è¯‘ (æ–°å¢é€»è¾‘)
        h3_chinese = soup.find('h3', string='æ–°æ¦‚å¿µè‹±è¯­ï¼ç¿»è¯‘')
        content['chinese'] = get_text_until_next_h3(h3_chinese)
            
        # 3. æå–å•è¯å’ŒçŸ­è¯­
        h3_vocab = soup.find('h3', string='æ–°æ¦‚å¿µè‹±è¯­ï¼å•è¯å’ŒçŸ­è¯­')
        # ç¬¬ä¸‰å†Œçš„ç”Ÿè¯æ ¼å¼å¯èƒ½ä¸ç¬¬äºŒå†Œä¸åŒï¼Œç”¨ \n åˆ†éš”æ›´åˆé€‚
        content['vocabulary'] = get_text_until_next_h3(h3_vocab).replace(" ", "\n")

        # åªè¦æ ¸å¿ƒå†…å®¹ä¸ä¸ºç©ºï¼Œå°±è®¤ä¸ºæœ¬æ¬¡çˆ¬å–æ˜¯åˆæ­¥æˆåŠŸçš„
        if content['english'] and content['chinese']:
            return content
        else:
            print(f"    - è­¦å‘Š: æœªèƒ½å®Œæ•´æŠ“å–åˆ° {lesson_url} çš„è‹±æ–‡æˆ–ä¸­æ–‡å†…å®¹ã€‚")
            return None
    except Exception:
        # éšè—å…·ä½“é”™è¯¯ï¼Œè®©è°ƒç”¨è€…å¤„ç†é‡è¯•
        return None

def process_single_lesson(lesson_num, book_num):
    """
    å¤„ç†å•ç¯‡è¯¾æ–‡çš„å®Œæ•´æµç¨‹ï¼šçˆ¬å– -> åœ¨çº¿æ ¡éªŒ -> å¤±è´¥é‡è¯• -> ä¿å­˜ã€‚
    è¿™æ˜¯è„šæœ¬çš„æ ¸å¿ƒâ€œè‡ªä¿®æ­£â€é€»è¾‘ã€‚
    """
    output_filepath = os.path.join(OUTPUT_DIR, f"lesson_{lesson_num:03d}.json")
    if os.path.exists(output_filepath):
        print(f"âœ… lesson_{lesson_num:03d}.json å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
        return True

    lesson_url = f"{BASE_URL}?id=course-{book_num}-{lesson_num:03d}"
    print(f"\n--- æ­£åœ¨å¤„ç† Lesson {lesson_num} ---")

    for attempt in range(1, MAX_ATTEMPTS + 1):
        print(f"  - ç¬¬ {attempt}/{MAX_ATTEMPTS} æ¬¡å°è¯•...")
        
        # é¦–æ¬¡çˆ¬å–
        main_data = scrape_nce_lesson(lesson_url)
        if not main_data:
            print("    - âŒ çˆ¬å–å¤±è´¥ï¼Œç¨åé‡è¯•...")
            time.sleep(2)
            continue

        # åœ¨çº¿æ ¡éªŒ (é€šè¿‡å†æ¬¡çˆ¬å–è¿›è¡Œæ¯”å¯¹)
        time.sleep(1) 
        verify_data = scrape_nce_lesson(lesson_url)
        if not verify_data:
            print("    - âŒ æ— æ³•è·å–æ ¡éªŒæ•°æ®ï¼Œé‡è¯•...")
            time.sleep(2)
            continue
            
        # å†…å®¹æ¯”å¯¹
        if (main_data['english'] == verify_data['english'] and
            main_data['chinese'] == verify_data['chinese'] and
            main_data['vocabulary'] == verify_data['vocabulary']):
            
            print("    - âœ… æ•°æ®ä¸€è‡´æ€§æ ¡éªŒé€šè¿‡ï¼å†…å®¹å®Œæ•´ã€‚")
            try:
                with open(output_filepath, 'w', encoding='utf-8') as f:
                    json.dump(main_data, f, ensure_ascii=False, indent=4)
                print(f"    - ğŸ’¾ å·²æˆåŠŸä¿å­˜åˆ°: {output_filepath}")
                return True
            except IOError as e:
                print(f"    - âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
                return False

        else:
            print("    - âŒ æ•°æ®ä¸ä¸€è‡´ï¼Œå¯èƒ½å­˜åœ¨ç½‘ç»œä¸ç¨³å®šæ€§ã€‚æ­£åœ¨é‡è¯•...")
            time.sleep(2)

    print(f"  - âŒ Lesson {lesson_num} åœ¨ {MAX_ATTEMPTS} æ¬¡å°è¯•åä»æ— æ³•ç¨³å®šæŠ“å–ï¼Œå·²è·³è¿‡ã€‚")
    return False

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼Œè´Ÿè´£è°ƒåº¦æ•´ä¸ªçˆ¬å–æµç¨‹"""
    print(f"ğŸš€ è„šæœ¬1ï¼šæ–°æ¦‚å¿µè‹±è¯­ç¬¬{BOOK_TO_SCRAPE}å†Œ è‡ªæˆ‘ä¿®æ­£çˆ¬è™« ğŸš€")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    failed_lessons = []
    for lesson_num in range(1, TOTAL_LESSONS + 1):
        success = process_single_lesson(lesson_num, BOOK_TO_SCRAPE)
        if not success:
            failed_lessons.append(lesson_num)
    
    print("\n" + "#"*50)
    print("ğŸ“Š æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆã€‚æœ€ç»ˆæŠ¥å‘Šï¼š")
    if not failed_lessons:
        print(f"âœ…ğŸ‰ğŸ‰ å®Œç¾ï¼æ‰€æœ‰ {TOTAL_LESSONS} ç¯‡è¯¾æ–‡éƒ½å·²æˆåŠŸæŠ“å–å¹¶é€šè¿‡æ ¡éªŒï¼")
    else:
        print(f"âŒ æ³¨æ„ï¼šæœ‰ {len(failed_lessons)} ç¯‡è¯¾æ–‡åœ¨å¤šæ¬¡å°è¯•åä¾ç„¶å¤±è´¥ã€‚")
        print(f"å¤±è´¥çš„è¯¾æ–‡ç¼–å·: {failed_lessons}")
    print("#"*50 + "\n")


if __name__ == '__main__':
    main()